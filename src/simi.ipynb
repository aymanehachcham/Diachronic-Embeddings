{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SENSES_FILE = 'embeddings_for_senses.json'\n",
    "EXAMPLES_FILE = 'embeddings_1980.json'\n",
    "WORDS = 'polysemous.txt'\n",
    "\n",
    "class LoadingEmbeddings():\n",
    "\n",
    "    @classmethod\n",
    "    def load_files(\n",
    "            cls,\n",
    "            root_dir:str,\n",
    "            sense_embeddings_file: str,\n",
    "            example_embeddings_file: str,\n",
    "        ):\n",
    "\n",
    "        if (os.path.exists(os.path.join(root_dir, sense_embeddings_file))) \\\n",
    "            and (os.path.exists(os.path.join(root_dir, sense_embeddings_file))):\n",
    "            cls.sense_file = os.path.join(root_dir, sense_embeddings_file)\n",
    "            cls.examples_file = os.path.join(root_dir, example_embeddings_file)\n",
    "\n",
    "        logging.basicConfig(level=logging.NOTSET)\n",
    "        ex, sens = cls._load_files()\n",
    "        return ex, sens\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_files():\n",
    "        with open(os.path.join('../embeddings', EXAMPLES_FILE), 'r') as f:\n",
    "            logging.info('{} Loading File {} {}'.format('-'*10, f.name, '-'*10))\n",
    "            example_embeds = json.load(f)\n",
    "\n",
    "        with open(os.path.join('../embeddings', SENSES_FILE), 'r') as f:\n",
    "            logging.info('{} Loading File {} {}'.format('-'*10, f.name, '-'*10))\n",
    "            senses_embeds = json.load(f)\n",
    "\n",
    "        return example_embeds, senses_embeds\n",
    "\n",
    "\n",
    "class Similarities():\n",
    "    def __init__(\n",
    "            self,\n",
    "        ):\n",
    "\n",
    "        self.embeddings_examples, self.embeddings_senses = LoadingEmbeddings.load_files(\n",
    "            root_dir='../embeddings',\n",
    "            sense_embeddings_file=SENSES_FILE,\n",
    "            example_embeddings_file=EXAMPLES_FILE\n",
    "        )\n",
    "        self.word_sense_proportions = {}\n",
    "\n",
    "    def _search_word_sense(self, word:str):\n",
    "        for w in self.embeddings_senses:\n",
    "            if w[0]['word'] == word:\n",
    "                yield w\n",
    "\n",
    "    def _cos_sim(self, vect_a:np.array, vect_b:np.array):\n",
    "        return (vect_a @ vect_b)/(norm(vect_a) * norm(vect_b))\n",
    "\n",
    "    def __call__(self, word:str):\n",
    "        from collections import Counter\n",
    "        examples = np.array(self.embeddings_examples[word]['embeddings'])\n",
    "        try:\n",
    "            senses = next(self._search_word_sense(word))\n",
    "        except StopIteration:\n",
    "            raise ValueError(\n",
    "                'Word not in list'\n",
    "            )\n",
    "\n",
    "        all_sims = []\n",
    "        for embed in examples:\n",
    "            s_argmax =  np.argmax(list(self._cos_sim(sens['embedding'], embed) for sens in senses))\n",
    "            all_sims.append(senses[s_argmax]['sense'])\n",
    "\n",
    "        self.word_sense_proportions['word'] = word\n",
    "        self.word_sense_proportions['props'] = list(map(lambda x: x[1]/len(all_sims), Counter(all_sims).most_common()))\n",
    "\n",
    "        return self.word_sense_proportions.copy()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:---------- Loading File ../embeddings/embeddings_1980.json ----------\n",
      "INFO:root:---------- Loading File ../embeddings/embeddings_for_senses.json ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'word': 'please',\n 'props': [0.5709876543209876, 0.404320987654321, 0.024691358024691357]}"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = Similarities()\n",
    "sim('please')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state\n",
      "right\n",
      "around\n",
      "black\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Word not in list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 69\u001B[0m, in \u001B[0;36mSimilarities.__call__\u001B[0;34m(self, word)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 69\u001B[0m     senses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_search_word_sense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "\u001B[0;31mStopIteration\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m w\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(word)\n\u001B[0;32m----> 6\u001B[0m     all_words\u001B[38;5;241m.\u001B[39mappend(\u001B[43msim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      8\u001B[0m all_words\n",
      "Cell \u001B[0;32mIn[35], line 71\u001B[0m, in \u001B[0;36mSimilarities.__call__\u001B[0;34m(self, word)\u001B[0m\n\u001B[1;32m     69\u001B[0m     senses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_search_word_sense(word))\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m---> 71\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWord not in list\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     73\u001B[0m     )\n\u001B[1;32m     75\u001B[0m all_sims \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m embed \u001B[38;5;129;01min\u001B[39;00m examples:\n",
      "\u001B[0;31mValueError\u001B[0m: Word not in list"
     ]
    }
   ],
   "source": [
    "with open('../data/target_words/polysemous.txt', 'r') as f: w = f.read()\n",
    "\n",
    "all_words = []\n",
    "for word in w.split('\\n'):\n",
    "    print(word)\n",
    "    all_words.append(sim(word))\n",
    "\n",
    "all_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:---------- Loading File ../embeddings/embeddings_1980.json ----------\n",
      "INFO:root:---------- Loading File ../embeddings/embeddings_for_senses.json ----------\n"
     ]
    }
   ],
   "source": [
    "embeddings_examples, embeddings_senses = LoadingEmbeddings.load_files(\n",
    "            root_dir='../embeddings',\n",
    "            sense_embeddings_file=SENSES_FILE,\n",
    "            example_embeddings_file=EXAMPLES_FILE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def _search_word_sense(word:str):\n",
    "    for w in embeddings_senses:\n",
    "        if w[0]['word'] == word:\n",
    "            yield w\n",
    "\n",
    "def _cos_sim(vect_a:np.array, vect_b:np.array):\n",
    "    return (vect_a @ vect_b)/(norm(vect_a) * norm(vect_b))\n",
    "\n",
    "examples = np.array(embeddings_examples['right']['embeddings'])\n",
    "try:\n",
    "    senses = next(_search_word_sense('right'))\n",
    "except StopIteration:\n",
    "        raise ValueError(\n",
    "            'Word not in list'\n",
    "        )\n",
    "\n",
    "all_sims = []\n",
    "for embed in examples:\n",
    "    s_argmax =  np.argmax(list(_cos_sim(sens['embedding'], embed) for sens in senses))\n",
    "    all_sims.append(senses[s_argmax]['sense'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.2855564557707888,\n 0.20608381822199306,\n 0.1939309125727333,\n 0.11747808794284452,\n 0.11327981144582751,\n 0.08367091404581277]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "list(map(lambda x: x[1]/len(all_sims), Counter(all_sims).most_common()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
